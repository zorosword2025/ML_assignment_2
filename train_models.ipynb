{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Assignment 2 - Model Training Notebook\n",
    "## Adult Income Prediction - Classification Models\n",
    "\n",
    "**BITS Pilani - M.Tech (AIML/DSE)**  \n",
    "**Course:** Machine Learning  \n",
    "**Assignment:** ML Assignment 2\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import kagglehub\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score, precision_score,\n",
    "    recall_score, f1_score, matthews_corrcoef,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "import pickle\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset from Kaggle\n",
    "print(\"Downloading Adult Income Prediction Dataset...\")\n",
    "path = kagglehub.dataset_download(\"mosapabdelghany/adult-income-prediction-dataset\")\n",
    "print(f\"Dataset downloaded to: {path}\")\n",
    "\n",
    "# Find CSV files\n",
    "import os\n",
    "csv_files = [f for f in os.listdir(path) if f.endswith('.csv')]\n",
    "print(f\"Available CSV files: {csv_files}\")\n",
    "\n",
    "# Load the dataset\n",
    "data_file = os.path.join(path, csv_files[0])\n",
    "df = pd.read_csv(data_file)\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information\n",
    "print(\"Dataset Info:\")\n",
    "print(f\"Rows: {df.shape[0]}\")\n",
    "print(f\"Columns: {df.shape[1]}\")\n",
    "print(f\"\\nColumn Names:\\n{df.columns.tolist()}\")\n",
    "print(f\"\\nData Types:\\n{df.dtypes}\")\n",
    "print(f\"\\nMissing Values:\\n{df.isnull().sum()}\")\n",
    "\n",
    "# Check for '?' as missing values\n",
    "print(\"\\nChecking for '?' as missing values...\")\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        missing = (df[col] == '?').sum()\n",
    "        if missing > 0:\n",
    "            print(f\"{col}: {missing} missing values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "print(\"Handling missing values...\")\n",
    "df = df.replace('?', np.nan)\n",
    "print(f\"Rows before removing NaN: {len(df)}\")\n",
    "df = df.dropna()\n",
    "print(f\"Rows after removing NaN: {len(df)}\")\n",
    "\n",
    "# Identify target column\n",
    "target_col = None\n",
    "for col in df.columns:\n",
    "    if 'income' in col.lower() or 'salary' in col.lower():\n",
    "        target_col = col\n",
    "        break\n",
    "\n",
    "if target_col is None:\n",
    "    target_col = df.columns[-1]\n",
    "\n",
    "print(f\"\\nTarget column: {target_col}\")\n",
    "print(f\"Target classes: {df[target_col].unique()}\")\n",
    "print(f\"Class distribution:\\n{df[target_col].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "# Encode target\n",
    "le_target = LabelEncoder()\n",
    "y = le_target.fit_transform(y)\n",
    "print(f\"Target encoding: {dict(zip(le_target.classes_, le_target.transform(le_target.classes_)))}\")\n",
    "\n",
    "# Identify column types\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "print(f\"\\nCategorical features ({len(categorical_cols)}): {categorical_cols}\")\n",
    "print(f\"Numerical features ({len(numerical_cols)}): {numerical_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "print(f\"\\n‚úÖ Encoding complete!\")\n",
    "print(f\"Final features: {X.shape[1]}\")\n",
    "print(f\"Total samples: {X.shape[0]}\")\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train-Test Split and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"\\nClass distribution in train set:\")\n",
    "print(pd.Series(y_train).value_counts())\n",
    "print(f\"\\nClass distribution in test set:\")\n",
    "print(pd.Series(y_test).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"‚úÖ Feature scaling complete!\")\n",
    "print(f\"Scaled training data shape: {X_train_scaled.shape}\")\n",
    "print(f\"Scaled test data shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize all 6 models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=10000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=5),\n",
    "    'K-Nearest Neighbor': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10),\n",
    "    'XGBoost': XGBClassifier(n_estimators=100, random_state=42, max_depth=5, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Models initialized:\")\n",
    "for name in models.keys():\n",
    "    print(f\"  ‚Ä¢ {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train and Evaluate Each Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "results = []\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING AND EVALUATING MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"AUC:       {auc:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "    print(f\"MCC:       {mcc:.4f}\")\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': round(accuracy, 4),\n",
    "        'AUC': round(auc, 4),\n",
    "        'Precision': round(precision, 4),\n",
    "        'Recall': round(recall, 4),\n",
    "        'F1': round(f1, 4),\n",
    "        'MCC': round(mcc, 4)\n",
    "    })\n",
    "    \n",
    "    # Save model\n",
    "    model_filename = f\"model_{model_name.lower().replace(' ', '_').replace('-', '_')}.pkl\"\n",
    "    with open(model_filename, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"‚úÖ Model saved: {model_filename}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ All models trained successfully!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('Accuracy', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS - ALL MODELS\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv('model_results.csv', index=False)\n",
    "print(\"\\n‚úÖ Results saved to model_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics = ['Accuracy', 'AUC', 'Precision', 'Recall', 'F1', 'MCC']\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
    "\n",
    "for idx, (ax, metric) in enumerate(zip(axes.flat, metrics)):\n",
    "    data = results_df.sort_values(metric, ascending=False)\n",
    "    ax.barh(data['Model'], data[metric], color=colors[idx])\n",
    "    ax.set_xlabel(metric, fontweight='bold')\n",
    "    ax.set_title(f'{metric} Comparison', fontweight='bold')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(data[metric]):\n",
    "        ax.text(v, i, f' {v:.3f}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualization saved as model_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Supporting Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save scaler\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(\"‚úÖ Scaler saved\")\n",
    "\n",
    "# Save label encoders\n",
    "with open('label_encoders.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoders, f)\n",
    "print(\"‚úÖ Label encoders saved\")\n",
    "\n",
    "# Save target encoder\n",
    "with open('target_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(le_target, f)\n",
    "print(\"‚úÖ Target encoder saved\")\n",
    "\n",
    "# Save feature names\n",
    "with open('feature_names.pkl', 'wb') as f:\n",
    "    pickle.dump(X.columns.tolist(), f)\n",
    "print(\"‚úÖ Feature names saved\")\n",
    "\n",
    "# Save test data sample for Streamlit\n",
    "test_data = pd.DataFrame(X_test_scaled[:1000], columns=X.columns)\n",
    "test_data['target'] = y_test[:1000]\n",
    "test_data.to_csv('test_data.csv', index=False)\n",
    "print(f\"‚úÖ Test data saved ({test_data.shape[0]} samples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Verify All Files Created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FILE VERIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "required_files = [\n",
    "    'model_logistic_regression.pkl',\n",
    "    'model_decision_tree.pkl',\n",
    "    'model_k_nearest_neighbor.pkl',\n",
    "    'model_naive_bayes.pkl',\n",
    "    'model_random_forest.pkl',\n",
    "    'model_xgboost.pkl',\n",
    "    'scaler.pkl',\n",
    "    'label_encoders.pkl',\n",
    "    'target_encoder.pkl',\n",
    "    'feature_names.pkl',\n",
    "    'test_data.csv',\n",
    "    'model_results.csv'\n",
    "]\n",
    "\n",
    "all_present = True\n",
    "for file in required_files:\n",
    "    if os.path.exists(file):\n",
    "        size = os.path.getsize(file)\n",
    "        print(f\"‚úÖ {file:40s} ({size:,} bytes)\")\n",
    "    else:\n",
    "        print(f\"‚ùå {file:40s} NOT FOUND\")\n",
    "        all_present = False\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "if all_present:\n",
    "    print(\"üéâ SUCCESS! All files created successfully!\")\n",
    "    print(\"You are ready to deploy on Streamlit!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Some files are missing. Please check the errors above.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Next Steps\n",
    "\n",
    "1. **Upload to GitHub:**\n",
    "   - Create a new repository\n",
    "   - Push all files including .pkl files\n",
    "\n",
    "2. **Deploy on Streamlit:**\n",
    "   - Go to share.streamlit.io\n",
    "   - Connect your GitHub repository\n",
    "   - Select app.py as main file\n",
    "   - Deploy!\n",
    "\n",
    "3. **Create Submission PDF:**\n",
    "   - Include GitHub link\n",
    "   - Include Streamlit app link\n",
    "   - Add screenshot from BITS Lab\n",
    "   - Copy README content\n",
    "\n",
    "4. **Submit on Taxila:**\n",
    "   - Upload the PDF before deadline\n",
    "   - Verify submission confirmation\n",
    "\n",
    "---\n",
    "\n",
    "**Good luck with your assignment! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
